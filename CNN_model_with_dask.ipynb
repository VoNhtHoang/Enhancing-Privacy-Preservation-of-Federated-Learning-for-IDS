{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "file_path = \"Processed_Data/Mapped_Dataset.csv\"\n",
    "\n",
    "df = dk.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_ml\\model_selection\\_split.py:464: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Global var \n",
    "batch_size = 512\n",
    "ratio_test_all = 0.15\n",
    "\n",
    "from dask_ml.model_selection import train_test_split \n",
    "# chia train test ratio 0.8:0.2 & random \n",
    "train_df, test_df = train_test_split(df, test_size=ratio_test_all, random_state=42)\n",
    "\n",
    "# # load từng batch\n",
    "def dask_to_tf_dataset(dask_df, batch_size=128, num_classes=10): \n",
    "    def generator():\n",
    "        for batch in dask_df.to_delayed():\n",
    "            batch=batch.compute()  \n",
    "            if batch.empty:\n",
    "                continue\n",
    "\n",
    "            X = batch.drop(columns='label').values.astype(np.float32)\n",
    "            y = batch['label'].values\n",
    "            y_onehot = to_categorical(y, num_classes=num_classes)  \n",
    "\n",
    "            num_splits = max(1, len(X) // batch_size)  # Đảm bảo không chia nhỏ quá mức\n",
    "            X_batches = np.array_split(X, num_splits)\n",
    "            y_batches = np.array_split(y_onehot, num_splits)\n",
    "\n",
    "            for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "                yield X_batch, y_batch\n",
    "                \n",
    "    output_signature = ( \n",
    "        tf.TensorSpec(shape=(None, 46), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None, 10), dtype=tf.int32),\n",
    "    )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# from functools import partial\n",
    "# def dask_generator(dask_df, batch_size=128, num_classes=10): \n",
    "#     for batch in dask_df.to_delayed():\n",
    "#         batch = batch.compute()  \n",
    "#         if batch.empty:\n",
    "#             continue\n",
    "\n",
    "#         X = batch.drop(columns='label').values.astype(np.float32)\n",
    "#         y = batch['label'].values\n",
    "#         y_onehot = to_categorical(y, num_classes=num_classes)  \n",
    "\n",
    "#         num_splits = max(1, len(X) // batch_size)\n",
    "#         X_batches = np.array_split(X, num_splits)\n",
    "#         y_batches = np.array_split(y_onehot, num_splits)\n",
    "\n",
    "#         for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "#             yield X_batch, y_batch \n",
    "\n",
    "# def dask_to_tf_dataset(dask_df, batch_size=128, num_classes=10): \n",
    "#     output_signature = ( \n",
    "#         tf.TensorSpec(shape=(None, dask_df.shape[1] - 1), dtype=tf.float32),  \n",
    "#         tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32),\n",
    "#     )\n",
    "\n",
    "#     return tf.data.Dataset.from_generator(\n",
    "#         partial(dask_generator, dask_df, batch_size, num_classes), \n",
    "#         output_signature=output_signature\n",
    "#     ).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = dask_to_tf_dataset(train_df, 512, 10).repeat()\n",
    "test_gen = dask_to_tf_dataset(test_df, 512, 10).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (46, 1)\n",
      "Epoch 1/10\n",
      "\u001b[1m75000/75000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1188s\u001b[0m 16ms/step - accuracy: 0.7567 - loss: 0.7000\n",
      "Epoch 2/10\n",
      "\u001b[1m75000/75000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1566s\u001b[0m 21ms/step - accuracy: 0.8222 - loss: 0.4666\n",
      "Epoch 3/10\n",
      "\u001b[1m75000/75000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1128s\u001b[0m 15ms/step - accuracy: 0.8595 - loss: 0.3516\n",
      "Epoch 4/10\n",
      "\u001b[1m75000/75000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1210s\u001b[0m 16ms/step - accuracy: 0.8017 - loss: 0.5447\n",
      "Epoch 5/10\n",
      "\u001b[1m75000/75000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1320s\u001b[0m 18ms/step - accuracy: 0.8407 - loss: 0.4234\n",
      "Epoch 6/10\n",
      "\u001b[1m75000/75000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14705s\u001b[0m 196ms/step - accuracy: 0.8522 - loss: 0.3854\n",
      "Epoch 7/10\n",
      "\u001b[1m75000/75000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2948s\u001b[0m 39ms/step - accuracy: 0.8842 - loss: 0.2759\n",
      "Epoch 8/10\n",
      "\u001b[1m75000/75000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3185s\u001b[0m 42ms/step - accuracy: 0.8648 - loss: 0.3399\n",
      "Epoch 9/10\n",
      "\u001b[1m22533/75000\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m42:12\u001b[0m 48ms/step - accuracy: 0.8671 - loss: 0.3143"
     ]
    }
   ],
   "source": [
    "\n",
    "# shape\n",
    "features, labels = next(iter(train_gen))\n",
    "input_shape = (features.shape[1], 1)\n",
    "output_shape = labels.shape[1]\n",
    "\n",
    "print(f\"Input Shape: {input_shape}\")\n",
    "\n",
    "from tensorflow import keras\n",
    "# Định nghĩa mô hình CNN\n",
    "# VGG, ...\n",
    "# Conv2D, tabular, ...\n",
    "# HE, tính tương thích của HE với CNN\n",
    "# Tính chất data in, out; Học tăng cường\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    layers.Conv1D(filters=64, kernel_size=3,  padding=\"same\",activation=\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(output_shape, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], use_multiprocessing=True)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     X_batch = batch[:, :-1]\n",
    "#     y_batch = batch[:, -1]\n",
    "#     y_onehot = to_categorical(y_batch, num_classes=10)\n",
    "    \n",
    "#     model.train_on_batch(X_batch, y_onehot, verbose=1)\n",
    "model.fit(train_gen, epochs=10, steps_per_epoch=75000, verbose = 1)\n",
    "\n",
    "# Lưu mô hình\n",
    "model.save(\"cnn_model_2-0_batch512_test015.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Để Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m190000/190000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5428s\u001b[0m 29ms/step - accuracy: 0.8963 - loss: 0.2570\n",
      "Loss: 0.8964436054229736 Acc: 0.8964436054229736\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model từ file .h5\n",
    "model = load_model(\"cnn_model_2-0_batch512_test015.h5\")\n",
    "\n",
    "# Test với dữ liệu đầu vào\n",
    "import numpy as np\n",
    "output = model.evaluate(test_gen, steps= 190000)\n",
    "print(f'Loss: {output[0]} Acc: {output[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
